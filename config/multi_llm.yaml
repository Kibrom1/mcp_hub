# Multi-LLM Configuration for MCP Hub

app:
  title: "MCP Hub â€” Multi-LLM Client"
  layout: "wide"
  debug: true

# LLM Provider Configuration
llm_providers:
  # OpenAI Configuration
  openai:
    enabled: true
    model: "gpt-4o-mini"
    max_tokens: 1000
    temperature: 0.3
    timeout: 30
  
  # Anthropic Configuration
  anthropic:
    enabled: false
    model: "claude-3-haiku-20240307"
    max_tokens: 1000
    temperature: 0.3
    timeout: 30
  
  # Google Gemini Configuration
  google:
    enabled: false
    model: "gemini-pro"
    max_tokens: 1000
    temperature: 0.3
    timeout: 30
  
  # Ollama Local Configuration
  ollama:
    enabled: false
    model: "llama2"
    base_url: "http://localhost:11434"
    max_tokens: 1000
    temperature: 0.3
    timeout: 60

# Default provider (will be auto-selected based on available API keys)
default_provider: "openai"

# Database configuration
database:
  url: "sqlite:///mcp.db"
  echo: false

# MCP Servers configuration
servers:
  - name: "filesystem"
    uri: "npx @modelcontextprotocol/server-filesystem /Users"
    enabled: true
  - name: "memory"
    uri: "npx @modelcontextprotocol/server-memory"
    enabled: true
  - name: "brave-search"
    uri: "npx @modelcontextprotocol/server-brave-search"
    enabled: false

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "mcp_hub.log"
  max_size: "10MB"
  backup_count: 5

# Security configuration
security:
  enable_auth: false
  session_timeout: 3600
  max_requests_per_minute: 60
